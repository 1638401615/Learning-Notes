# AMP

## Introduction

* 为异构的模型和集群自动找到最佳的模型并行策略

  难点在于：

  * 模型具有不同类型的层，表现出不同的执行和通信开销，现有的层分配的启发式方法会失效
  * 在大型集群上评估一个模型的并行策略代价可能是昂贵的

* 本文提出了一个一种多项式时间的动态规划算法和在有限预算下评估大量策略的新的成本模型

## Background and Related Work

Data-parallel strategies

* 把输入数据批平均分配给worker，每个worker都有完整的模型副本
* 每次迭代每个worker算出自己的梯度，然后在下一次迭代前进行同步
* DP需要每个worker都有一个完整的模型副本，不能直接用于训练具有大量参数的模型

Tensor model-parallel strategies

* 在TMP中，每两个连续层的层权值先按行(即输入维度)划分，然后按列(即输出维度)划分
* TMP消除了同步第一层中间输出的需要，但是之后需要大量的跨设备通信
* TMP通常和DP结合提高训练的吞吐量

Pipeline-parallel strategies

* 把layers放置在gpu上，mini-batch被分成了更小的micro-batch
* 向前和向后的计算在微批之间被流水线化
* PP比DP和TMP需要更少的通信，但是存在设备空闲

3D parallelism

* DP，MP，PP共同使用，权衡可伸缩性、内存占用和设备利用率



# FedLamp

Adaptive Control of Local Updating and Model Compression for Efficient Federated Learning

## Introduction 

* 解决本地资源限制的一个普遍方法：本地多训练几轮，以此减少通信轮次，减低通信开销

文章贡献：

* 提出FedLamp，集成局部更新的自适应控制和模型压缩，克服资源受限的边缘系统的系统异构性和上下文动态变化

* 理论分析了模型的收敛速度，得到和**局部更新频率和模型压缩比相关的收敛上界**
* 提出了针对不同边缘节点，自适应确定不同且合适的局部更新频率和模型压缩比的控制算法，减少等待时间，提高训练效率
