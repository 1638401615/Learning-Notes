服务器通过socket监听来自用户的请求：

1. 创建监听socket文件描述符
2. 创建监听socket的TCP/IP 的IPv4 socket地址
3. 绑定socket和它的地址
4. 创建监听队列以存放待处理的客户链接

> 监听的socket和通信的socket是不一样的



## select、poll、epoll

* I/O多路复用最大优势：系统开销小，不需要创建多个进程/线程，一个进程可以监视多个描述符
* 本质上select、poll、epoll都是**同步I/O**，因为他们都要在事件就绪后**自己负责进行读写**，这个读写过程是阻塞的

### select

* fd_set就是一个bitmap，每一位标志着一个fd的状态。
* select的流程：
  1. 先清空fd_set
  2. 将要测试的fd加入fd_set
  3. 调用select测试fd_set中所有fd，有事件返回没救更新fd_set中的位，把没事件的fd清除
  4. 用FD_ISSET检查某个fd在select后，相应位是否仍为1，若是1说明有事件发生

### poll

* 用数组去存描述符，打破最大文件描述符限制。
* poll在用户态使用数组方式传递文件描述符，内核中会转为链表方式存储

### epoll

```c++
1. 创建 epoll 的句柄，它本身就是一个 fd
int epoll_create(int size);

2. 注册需要监视 fd 和事件
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

3. 等待事件发生
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

* 用红黑树存储事件，保证对连接的高效插入、查找和删除
* 包含就绪的描述符链表和等待队列链表。有连接就绪，内核就把就绪连接放到就绪链表。无需去遍历红黑树找。、

* 设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程

epoll_create的源码实现：

```cpp
SYSCALL_DEFINE1(epoll_create1, int, flags)
{
    struct eventpoll *ep = NULL;

    //创建一个 eventpoll 对象
    error = ep_alloc(&ep);
}

//struct eventpoll 的定义
// file：fs/eventpoll.c
struct eventpoll {

    //sys_epoll_wait用到的等待队列
    wait_queue_head_t wq;

    //接收就绪的描述符都会放到这里
    struct list_head rdllist;

    //每个epoll对象中都有一颗红黑树
    struct rb_root rbr;

    ......
}
static int ep_alloc(struct eventpoll **pep)
{
    struct eventpoll *ep;

    //申请 epollevent 内存
    ep = kzalloc(sizeof(*ep), GFP_KERNEL);

    //初始化等待队列头
    init_waitqueue_head(&ep->wq);

    //初始化就绪列表
    INIT_LIST_HEAD(&ep->rdllist);

    //初始化红黑树指针
    ep->rbr = RB_ROOT;

    ......
}
```

- **wq：** 等待队列链表。软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户进程。
  - 当服务器程序调用epoll_wait时，如果没有事件发生，就让服务器程序阻塞在epoll_wait上。这时，服务器程序会被加入到一个等待队列中，等待内核的唤醒 。
  - 当内核检测到某个套接字有事件发生时，会触发相应的回调函数，将其对应的epoll_item添加到就绪链表中，并唤醒阻塞在epoll_wait上的服务器程序。这时，服务器程序会被从等待队列中移除，并继续执行 。
- **rbr：** 红黑树。为了支持对海量连接的高效查找、插入和删除，eventpoll 内部使用的就是红黑树。通过红黑树来管理用户主进程accept添加进来的所有 socket 连接。
- **rdllist：** 就绪的描述符链表。当有连接就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样应用进程只需要判断链表就能找出就绪进程，而不用去遍历红黑树的所有节点了。

> 回调函数什么时候用的？
>
> * 内核为每个被监控的套接字注册的一个函数，**当有事件发生，触发该回调函数**，然后将其对应的epoll_item**加入就绪链表**，并唤醒阻塞在epoll_wait上的用户进程。

![img](https://pica.zhimg.com/80/v2-adf545de491e45398a0e4b5fb1ec4a98_720w.webp?source=1940ef5c)

|              | select                                                       | poll                                                         | epoll                                                 |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------------------- |
| 性能         | 随着连接数的增加，性能急剧下降，处理成千上万的并发连接数时，性能很差 | 随着连接数的增加，性能急剧下降，处理成千上万的并发连接数时，性能很差 | 随着连接数的增加，性能基本没有变化                    |
| 连接数       | 一般1024                                                     | 无限制                                                       | 无限制                                                |
| 内存拷贝     | 每次调用select拷贝                                           | 每次调用poll拷贝                                             | **fd首次调用epoll_ctl拷贝，每次调用epoll_wait不拷贝** |
| 数据结构     | bitmap                                                       | 数组                                                         | 红黑树                                                |
| 内在处理机制 | 线性轮询                                                     | 线性轮询                                                     | FD挂在红黑树，通过事件回调callback                    |
| 时间复杂度   | O(n)                                                         | O(n)                                                         | O(1)                                                  |

## 线程池

**:star:线程池中的线程数量是依据什么确定的？**

> 在StackOverflow上面发现了一个还不错的[回答](https://stackoverflow.com/a/16128493/7121726)，意思是：
> 线程池中的线程数量最直接的限制因素是中央处理器(CPU)的处理器(processors/cores)的数量`N`：如果你的CPU是4-cores的，对于**CPU密集型**的任务(如视频剪辑等消耗CPU计算资源的任务)来说，那线程池中的**线程数量最好也设置为4（或者+1防止其他因素造成的线程阻塞）**；对于**IO密集型**的任务，一般要多于CPU的核数，因为**线程间竞争的不是CPU的计算资源而是IO**，IO的处理一般较慢，**多于cores数的线程将为CPU争取更多的任务**，不至在线程处理IO的过程造成CPU空闲导致资源浪费，公式：`最佳线程数 = CPU当前可使用的Cores数 * 当前CPU的利用率 * (1 + CPU等待时间 / CPU处理时间)`（还有回答里面提到的Amdahl准则可以了解一下）



### Get vs Post

* Get**参数**在URL，Post参数在request body
* GET请求参数会被完整**保留在浏览器历史记录**里，而POST中的参数不会被保留。
* GET请求在URL中传送的**参数是有长度限制**。（大多数）浏览器通常都会限制url长度在2K个字节，而（大多数）服务器最多处理64K大小的url。
* **GET产生一个TCP数据包；POST产生两个TCP数据包。**对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；而对于POST，浏览器**先发送header**，服务器响应100（指示信息—表示请求已接收，继续处理）continue，浏览器**再发送data**，服务器响应200 ok（返回数据）。



## 线程同步机制封装类

RAII：Resource Acquisition is Initialization

* 将资源或者状态与对象的生命周期绑定，通过C++语言机制，实现资源和状态的安全管理，智能指针是最好的例子

> 同步/异步 VS 阻塞/非阻塞
>
> - 同步异步IO是指**应用程序和内核之间的交互方式，关注的是消息通信机制**，同步IO是指应用程序需要等待IO操作完成后才能继续执行，异步IO是指应用程序不需要等待IO操作完成，而是在IO操作完成后得到通知。
> - 阻塞非阻塞是指**系统调用的返回方式，关注的是程序等待消息通知时候的状态**，阻塞是指系统调用在等待IO事件发生时挂起进程或线程，直到有结果返回，非阻塞是指系统调用在等待IO事件发生时立即返回一个错误码，不挂起进程或线程。
>
> **同步阻塞**：小明一直盯着下载进度条，到 100% 的时候就完成。 - 同步体现在：等待下载完成通知。 - 阻塞体现在：等待下载完成通知过程中，不能做其他任务处理。
>
> **同步非阻塞**：小明提交下载任务后就去干别的，每过一段时间就去瞄一眼进度条，看到 100% 就完成。 - 同步体现在：等待下载完成通知。 - 非阻塞体现在：等待下载完成通知过程中，去干别的任务了，只是时不时会瞄一眼进度条。【小明必须要在两个任务间切换，关注下载进度】
>
> **异步阻塞**：小明换了个有下载完成通知功能的软件，下载完成就“叮”一声。不过小明不做别的事，仍然一直等待“叮”的声音。 - 异步体现在：下载完成“叮”一声通知。 - 阻塞体现在：等待下载完成“叮”一声通知过程中，不能做其他任务处理。
>
> **异步非阻塞**：仍然是那个会“叮”一声的下载软件，小明提交下载任务后就去干别的，听到“叮”的一声就知道完成了。 - 异步体现在：下载完成“叮”一声通知。 - 非阻塞体现在：等待下载完成“叮”一声通知过程中，去干别的任务了，只需要接收“叮”声通知即可。【软件处理下载任务，小明处理其他任务，不需关注进度，只需接收软件“叮”声通知，即可】
>
> * 这里的下载就是kernel在完成，下载就是IO操作。
>
> * 同步/异步是“下载完成消息”通知的方式（机制），而阻塞/非阻塞则是在等待“下载完成消息”通知过程中的状态（能不能干其他任务）